# Streaming Observability Platform

A production-style, local streaming observability platform that demonstrates real-time telemetry processing, anomaly detection, and LLM-based incident explanation generation.

## ğŸ¯ Overview

This project showcases senior data engineering skills through a complete, working observability pipeline that:
- Ingests and processes service telemetry in near-real-time
- Performs windowed aggregations using Spark Structured Streaming
- Detects anomalies using rule-based algorithms
- Generates human-readable incident explanations using a local LLM
- Orchestrates workflows with Apache Airflow
- Visualizes metrics and incidents in an interactive dashboard

**Key Constraints:**
- âœ… 100% local execution (no cloud services)
- âœ… Runs entirely via Podman Compose
- âœ… Uses local LLM only (Ollama with Phi model)
- âœ… Production-reasonable architecture and code quality

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Event Generator â”‚ (Python - simulates service telemetry)
â”‚  - checkout     â”‚
â”‚  - payments     â”‚
â”‚  - search       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Kafka (3 topics)                       â”‚
â”‚  - raw_events                                       â”‚
â”‚  - aggregated_metrics                               â”‚
â”‚  - anomalies                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Spark Structured Streaming                      â”‚
â”‚  - 1-minute tumbling windows                         â”‚
â”‚  - Aggregations per service:                         â”‚
â”‚    â€¢ request_count, error_rate                       â”‚
â”‚    â€¢ avg_latency, p50, p95, p99                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DuckDB (Metrics Store)                  â”‚
â”‚  Tables:                                             â”‚
â”‚  - service_metrics                                   â”‚
â”‚  - anomalies                                         â”‚
â”‚  - incident_summaries                                â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚
       â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Anomaly Detectorâ”‚  â”‚   LLM Narrator           â”‚
â”‚  (Rule-based)   â”‚  â”‚  (Ollama - Phi model)    â”‚
â”‚  - Error surge  â”‚  â”‚  - Reads aggregated data â”‚
â”‚  - Latency spikeâ”‚  â”‚  - Generates summaries   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Apache Airflow (Orchestration)            â”‚
â”‚  DAGs:                                               â”‚
â”‚  - monitoring_dag: Health checks every 5 min         â”‚
â”‚  - data_quality_dag: Validation & cleanup every 6h   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Streamlit Dashboard                          â”‚
â”‚  - Real-time metrics visualization                   â”‚
â”‚  - Anomaly timeline                                  â”‚
â”‚  - LLM-generated incident summaries                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ docker-compose.yml          # Podman Compose configuration
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env.example               # Environment variables template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ data/                      # Database and checkpoints (generated)
â”‚   â”œâ”€â”€ observability.db       # DuckDB database
â”‚   â””â”€â”€ checkpoints/           # Spark checkpoints
â”‚
â”œâ”€â”€ event_generator/           # Telemetry event simulator
â”‚   â”œâ”€â”€ generator.py           # Event generation logic
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ spark/                     # Spark Structured Streaming
â”‚   â””â”€â”€ streaming_job.py       # Main streaming job
â”‚
â”œâ”€â”€ anomaly_detection/         # Anomaly detection service
â”‚   â”œâ”€â”€ detector.py            # Rule-based detector
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ llm/                       # LLM narrator service
â”‚   â”œâ”€â”€ narrator.py            # Incident explanation generator
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ airflow/                   # Airflow orchestration
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ monitoring_dag.py  # Health monitoring
â”‚   â”‚   â””â”€â”€ data_quality_dag.py # Data validation
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements-airflow.txt
â”‚
â”œâ”€â”€ dashboard/                 # Streamlit dashboard
â”‚   â”œâ”€â”€ app.py                 # Main dashboard app
â”‚   â””â”€â”€ Dockerfile
â”‚
â””â”€â”€ scripts/                   # Utility scripts
    â”œâ”€â”€ init.sh                # Initialization script
    â”œâ”€â”€ init_db.py             # Database schema setup
    â”œâ”€â”€ health_check.sh        # Service health check
    â””â”€â”€ shutdown.sh            # Cleanup script
```

## ğŸš€ Quick Start

**Choose your setup mode:**

### ğŸ³ Production Mode (Recommended for First Run)

Run the complete platform in containers - perfect for demos and portfolio showcase.

**Prerequisites:**
- **Podman** and **Podman Compose** installed
- **8GB+ RAM** (recommended)
- **10GB+ disk space**

**Setup:**

1. **Clone the repository:**
   ```bash
   git clone <repository-url>
   cd streaming-observability-with-llm
   ```

2. **Initialize and start:**
   ```bash
   make init    # Create directories and database
   make start   # Start all services
   ```

3. **Check status** (~2-3 minutes for initialization):
   ```bash
   make health
   ```

4. **Access the platform:**
   - **Dashboard:** http://localhost:8501
   - **Airflow:** http://localhost:8080 (admin/admin)

See [QUICKSTART.md](QUICKSTART.md) for detailed production setup.

---

### ğŸ’» Development Mode (For Code Changes)

Run services locally with Python venv - perfect for development and testing.

**Prerequisites:**
- **Python 3.11+**
- **UV package manager** (installed automatically)

**Setup:**

1. **Setup development environment:**
   ```bash
   make dev-setup
   ```
   This installs UV, creates a virtual environment, and installs all dependencies (fast!).

2. **Activate the environment:**
   ```bash
   source .venv/bin/activate
   ```

3. **Run individual services:**
   ```bash
   # Start supporting services (Kafka, Ollama)
   podman-compose up -d kafka ollama
   

   # Download the LLM model (first time only, ~1.6GB)
   podman-compose up -d ollama-init
   
   # Wait for model download to complete
   podman-compose logs -f ollama-init
   
   # Run services locally
   python event_generator/generator.py
   python anomaly_detection/detector.py
   streamlit run dashboard/app.py
   ```

See [DEVELOPMENT.md](DEVELOPMENT.md) for complete development workflow.

---

### ğŸ“Š Key Differences

| Feature | Production Mode ğŸ³ | Development Mode ğŸ’» |
|---------|-------------------|-------------------|
| **Setup** | `make start` | `make dev-setup` |
| **Isolation** | Containers | Python venv |
| **Use Case** | Complete platform | Individual services |
| **Iteration Speed** | Slower (rebuild containers) | Fast (direct code changes) |
| **Resource Usage** | ~5GB RAM | ~2GB RAM |
| **Best For** | Demos, portfolio | Development, debugging |

---

## ğŸ® Usage

### Viewing the Dashboard

Navigate to http://localhost:8501 to see:
- **Real-time metrics** (request rate, error rate, latency)
- **Anomaly timeline** (color-coded by severity)
- **LLM-generated incident summaries**

The dashboard auto-refreshes every 30 seconds.

### Monitoring with Airflow

Access Airflow at http://localhost:8080:
- **monitoring_dag**: Runs every 5 minutes to check pipeline health
- **data_quality_dag**: Runs every 6 hours for data validation and cleanup

### Checking Service Health

```bash
./scripts/health_check.sh
```

This shows:
- Service status (Kafka, Ollama, Airflow, Dashboard)
- Database record counts

### Viewing Logs

```bash
# All services
podman-compose logs -f

# Specific service
podman-compose logs -f event-generator
podman-compose logs -f spark-streaming
podman-compose logs -f anomaly-detector
podman-compose logs -f llm-narrator
```

### Stopping the Platform

```bash
podman-compose down
```

Or use the provided script:
```bash
./scripts/shutdown.sh
```

Data persists in `./data/` directory.

## ğŸ”§ Configuration

### Environment Variables

Copy `.env.example` to `.env` and modify as needed:

```bash
# Kafka
KAFKA_BROKER=kafka:9092
KAFKA_RAW_EVENTS_TOPIC=raw_events

# DuckDB
DUCKDB_PATH=/data/observability.db

# LLM
OLLAMA_URL=http://ollama:11434
OLLAMA_MODEL=phi

# Event Generation
ANOMALY_PROBABILITY=0.05  # 5% chance of anomaly injection
EVENTS_PER_SECOND=10
```

### Anomaly Detection Thresholds

Thresholds are defined in `anomaly_detection/detector.py`:

```python
ERROR_RATE_MULTIPLIER = 3.0  # Alert if 3x baseline
LATENCY_THRESHOLDS = {
    'checkout': 300,   # 300ms SLA
    'payments': 400,
    'search': 150
}
```

## ğŸ“Š Data Flow

1. **Event Generation**: Simulates 10 events/second across 3 services
2. **Kafka Ingestion**: Events published to `raw_events` topic
3. **Spark Processing**: 
   - 1-minute tumbling windows
   - Aggregations computed per service
   - Results written to DuckDB
4. **Anomaly Detection**:
   - Runs every 60 seconds
   - Compares metrics against baselines
   - Records anomalies to database
5. **LLM Narration**:
   - Detects unprocessed anomalies
   - Builds context from aggregated metrics
   - Calls local Ollama API for explanation
   - Stores summaries in database
6. **Visualization**: Dashboard queries DuckDB and renders charts

## ğŸ§ª Testing Anomalies

The event generator automatically injects anomalies with configurable probability (default 5%). To manually trigger more anomalies:

1. Increase anomaly probability:
   ```bash
   # Edit docker-compose.yml
   # Change: ANOMALY_PROBABILITY: 0.05 to 0.20
   podman-compose restart event-generator
   ```

2. Watch for anomalies:
   ```bash
   podman-compose logs -f anomaly-detector
   podman-compose logs -f llm-narrator
   ```

3. View in dashboard at http://localhost:8501

## ğŸ† Design Decisions & Trade-offs

### Why These Technologies?

- **Kafka**: Industry standard for event streaming, handles backpressure well
- **Spark Structured Streaming**: Production-grade streaming engine with excellent windowing support
- **DuckDB**: Embedded analytical database, perfect for local execution, fast aggregations
- **Airflow**: De facto standard for workflow orchestration
- **Ollama + Phi**: Local LLM that runs on laptops, responsible AI usage (small model, minimal calls)
- **Streamlit**: Rapid dashboard development, Python-native

### Key Trade-offs

| Decision | Trade-off | Rationale |
|----------|-----------|-----------|
| Rule-based anomaly detection | Less sophisticated than ML | Simple, explainable, no training data needed |
| 1-minute windows | Not sub-second precision | Balances latency vs. computational load |
| DuckDB vs. PostgreSQL | Single-node only | Simpler setup, faster for analytics |
| Local LLM (Phi) | Less capable than GPT-4 | Privacy, cost, local execution |
| Ollama API calls | Slight latency | More flexible than embedding models |

### Production Considerations

If deploying to production:
- Replace DuckDB with distributed store (ClickHouse, TimescaleDB)
- Add Kafka cluster with replication
- Implement ML-based anomaly detection
- Use larger LLM or cloud API (with rate limiting)
- Add authentication and RBAC
- Implement alerting (PagerDuty, Slack)
- Add distributed tracing (Jaeger)

## ğŸ“ˆ Metrics & Observability

The platform generates the following metrics per service:

- **Request Count**: Total requests in window
- **Error Count**: HTTP 4xx/5xx responses
- **Error Rate**: Percentage of failed requests
- **Avg Latency**: Mean response time (ms)
- **P50/P95/P99 Latency**: Percentile latencies

Anomalies detected:
- **Error Surge**: Error rate > 3x baseline
- **Latency Spike**: Avg latency > 2x SLA threshold

## ğŸ§¹ Maintenance

### Data Retention

The `data_quality_dag` automatically cleans up:
- Metrics older than 7 days
- Resolved anomalies older than 7 days
- Incident summaries older than 7 days

### Manual Cleanup

```bash
# Remove database
rm ./data/observability.db

# Remove checkpoints
rm -rf ./data/checkpoints/*

# Remove logs
rm -rf ./data/logs/*
```

### Resetting the Platform

```bash
podman-compose down
rm -rf ./data/*
./scripts/init.sh
podman-compose up -d
```

## ğŸ› Troubleshooting

### Services Not Starting

```bash
# Check service status
podman-compose ps

# View logs
podman-compose logs <service-name>
```

### No Metrics in Dashboard

1. Check Kafka is receiving events:
   ```bash
   podman-compose logs event-generator
   ```

2. Check Spark is processing:
   ```bash
   podman-compose logs spark-streaming
   ```

3. Verify database has data:
   ```bash
   python3 scripts/health_check.sh
   ```

### LLM Not Generating Summaries

1. Check Ollama is running:
   ```bash
   curl http://localhost:11434/api/tags
   ```

2. Verify model is pulled:
   ```bash
   podman-compose logs ollama-init
   ```

3. Check narrator logs:
   ```bash
   podman-compose logs llm-narrator
   ```

### Ollama Model Download Fails (Corporate Proxy/VPN)

**Symptoms:**
- `ollama-init` logs show: `dial tcp: lookup registry.ollama.ai: no such host`
- Ollama container is "unhealthy"
- Model download fails repeatedly

**Solutions:**

1. **Configure proxy settings** in `.env`:
   ```bash
   HTTP_PROXY=http://your-proxy.company.com:8080
   HTTPS_PROXY=http://your-proxy.company.com:8080
   NO_PROXY=localhost,127.0.0.1,kafka,ollama,zookeeper
   ```

2. **Update docker-compose.yml** with proxy settings for ollama-init service (see docker-compose.yml lines 93-101)

3. **If on corporate VPN**: Try disconnecting VPN temporarily for model download:
   ```bash
   # Disconnect VPN
   podman rm -f ollama-init
   podman-compose up -d ollama-init
   podman logs -f ollama-init  # Watch download progress
   # Reconnect VPN after download completes
   ```

4. **Restart containers** after updating settings:
   ```bash
   podman-compose down
   podman-compose up -d kafka ollama
   podman-compose up -d ollama-init
   ```

### High Memory Usage

- Reduce Spark parallelism in `docker-compose.yml`:
  ```yaml
  --executor-memory 512m
  --driver-memory 512m
  ```

- Reduce event generation rate:
  ```yaml
  EVENTS_PER_SECOND: 5
  ```

## ğŸ¤ Contributing

This is a portfolio project, but suggestions are welcome! Please open an issue for discussion.

## ğŸ“œ License

MIT License - See LICENSE file for details

## ğŸ‘¤ Author

**Your Name**
- LinkedIn: [https://www.linkedin.com/in/modi-ashutosh/]
- GitHub: [https://github.com/modiashu]

## ğŸ™ Acknowledgments

- Apache Spark, Kafka, and Airflow communities
- Ollama for local LLM hosting
- Streamlit for rapid dashboard development

---
